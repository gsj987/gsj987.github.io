<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine-learning on gsj987的博客</title>
    <link>https://gsj987.github.io/categories/machine-learning/</link>
    <description>Recent content in machine-learning on gsj987的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Mon, 09 Mar 2020 00:00:00 +0800</lastBuildDate>
    
	<atom:link href="https://gsj987.github.io/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Tensorflow 体验篇-4 卷积网络和图像分类</title>
      <link>https://gsj987.github.io/posts/tensorflow-experience-4/</link>
      <pubDate>Mon, 09 Mar 2020 00:00:00 +0800</pubDate>
      
      <guid>https://gsj987.github.io/posts/tensorflow-experience-4/</guid>
      <description>从理论上说，全连接神经网络能够表达一切的多项式，只要深度（隐藏层）够深，全连接神经网络能适应所有的模型，但缺点是需要极大的计算量，而且在深度增加，激活函数的传递量会大大减弱。卷积神经网络是为了解决这个问题，他利用类似生物的局部兴奋逐层整合，形成高级的抽象的概念的过程，减少训练参数，但增加深度，增加对抽象概念的准确性。CNN 在图像领域使用较多。</description>
    </item>
    
    <item>
      <title>Tensorflow 体验篇-3 全连接网络</title>
      <link>https://gsj987.github.io/posts/tensorflow-experience-3/</link>
      <pubDate>Mon, 07 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://gsj987.github.io/posts/tensorflow-experience-3/</guid>
      <description>全连接网络(fully connected network)是神经网络中非常重要的概念，他是由多层网络输入输出所组成的节点映射，每一层之间的节点都相互连接，对于一个 $R^m-&amp;gt;R^n$ 的全连接网络，输入(m 个节点） 和输出（n 个节点）之间可能会有多个隐藏层，如图所示：</description>
    </item>
    
    <item>
      <title>Tensorflow 体验篇-2 使用 Tensorflow 做逻辑回归</title>
      <link>https://gsj987.github.io/posts/tensorflow-experience-2/</link>
      <pubDate>Fri, 03 May 2019 00:00:00 +0800</pubDate>
      
      <guid>https://gsj987.github.io/posts/tensorflow-experience-2/</guid>
      <description>逻辑回归 逻辑回归是机器学习中最重要的概念之一。所谓逻辑回归就是在几组数据集中，利用广义线性回归的方法，对数据进行分类的方法。虽然他叫回归，但本质上是利用回归方法做了分类。其使用前提是：</description>
    </item>
    
    <item>
      <title>Tensorflow 体验篇-1 使用 Tensorflow 做线性回归</title>
      <link>https://gsj987.github.io/posts/tensorflow-experience-1/</link>
      <pubDate>Fri, 26 Apr 2019 00:00:00 +0800</pubDate>
      
      <guid>https://gsj987.github.io/posts/tensorflow-experience-1/</guid>
      <description>线性回归是非常基础的统计学知识，也是所有机器学习研究的源头，因为现在的数学都还没有能很好的解决非线性的问题，所以基本上所有的机器学习的思路都是用非线性核函数，将数据转换到近似线性的空间中，然后再用线性方法如线性回归进行解决。</description>
    </item>
    
    <item>
      <title>Tensorflow 体验篇-0 安装</title>
      <link>https://gsj987.github.io/posts/tensorflow-experience-0/</link>
      <pubDate>Sun, 21 Apr 2019 00:00:00 +0800</pubDate>
      
      <guid>https://gsj987.github.io/posts/tensorflow-experience-0/</guid>
      <description>综述 当下机器学习火热，作为4年前也算是学过一些的研究生（半路出家），也很好奇现在的主流工业界对ML已经发展到什么程度了。在看了《精通数据科学：从线性回归到深度学习》1，以及简单的翻阅了几页的《TensorFlow for Deep Learning》2，我觉得现代深度学习的应用还算简单，比我们读书的时候新思路是很多，但是鉴于工业界的发展，实践也容易多了。</description>
    </item>
    
  </channel>
</rss>